{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1rGlkoiaxBc7cDmAxxDmspiRND7w_KMoE","authorship_tag":"ABX9TyPE7jV2iq/yWCNmcYnzYNaJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["! pip install langchain-community pinecone -q"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GiEWZCuD-cpi","executionInfo":{"status":"ok","timestamp":1736742279037,"user_tz":-330,"elapsed":8244,"user":{"displayName":"M Ihshan","userId":"14788756344858455673"}},"outputId":"87e5d35e-47df-4aeb-d139-a03a7d947ac8"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m2.1/2.5 MB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m427.3/427.3 kB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","execution_count":39,"metadata":{"id":"9yHVMTn62sFS","executionInfo":{"status":"ok","timestamp":1736749523918,"user_tz":-330,"elapsed":396,"user":{"displayName":"M Ihshan","userId":"14788756344858455673"}}},"outputs":[],"source":["from langchain.llms import OpenAI, HuggingFaceHub\n","from langchain.embeddings import OpenAIEmbeddings, HuggingFaceEmbeddings\n","from langchain.chains import LLMChain, RetrievalQA\n","from langchain.prompts import PromptTemplate\n","from langchain.vectorstores import Pinecone\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain.document_loaders import (\n","    PyPDFLoader,\n","    TextLoader,\n","    CSVLoader,\n","    UnstructuredWordDocumentLoader\n",")\n","from langchain.memory import ConversationBufferMemory\n","from typing import List, Dict, Optional, Union, Any\n","import pinecone\n","import os\n","import logging\n","from pathlib import Path\n","from abc import ABC, abstractmethod"]},{"cell_type":"code","source":["class ModelLocation(ABC):\n","    \"\"\"Strategy for model location handling\"\"\"\n","    @abstractmethod\n","    def get_model(self, model_name: str):\n","        pass\n","\n","class RemoteModelLocation(ModelLocation):\n","    \"\"\"Handles remote API-based models\"\"\"\n","    def get_model(self, model_name: str):\n","        return {\"location\": \"remote\", \"model_name\": model_name}\n","\n","class LocalModelLocation(ModelLocation):\n","    \"\"\"Handles local model loading\"\"\"\n","    def __init__(self, cache_dir: Optional[str] = None):\n","        self.cache_dir = cache_dir\n","\n","    def get_model(self, model_name: str):\n","        return {\n","            \"location\": \"local\",\n","            \"model_name\": model_name,\n","            \"cache_dir\": self.cache_dir\n","        }"],"metadata":{"id":"audJU-VC_Pjp","executionInfo":{"status":"ok","timestamp":1736743267512,"user_tz":-330,"elapsed":368,"user":{"displayName":"M Ihshan","userId":"14788756344858455673"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["class BaseLLMProvider(ABC):\n","    \"\"\"Abstract base class for LLM providers\"\"\"\n","\n","    def __init__(self, model_location: ModelLocation):\n","        self.model_location = model_location\n","\n","    @abstractmethod\n","    def get_llm(self):\n","        pass\n","\n","    @abstractmethod\n","    def get_embeddings(self):\n","        pass"],"metadata":{"id":"mAe2yawJ_SS4","executionInfo":{"status":"ok","timestamp":1736745077067,"user_tz":-330,"elapsed":416,"user":{"displayName":"M Ihshan","userId":"14788756344858455673"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["class OpenAIProvider(BaseLLMProvider):\n","    \"\"\"OpenAI implementation - Always remote\"\"\"\n","\n","    def __init__(self, model_name: str = \"gpt-3.5-turbo\", temperature: float = 0.7):\n","        super().__init__(RemoteModelLocation())  # OpenAI is always remote\n","        self.model_name = model_name\n","        self.temperature = temperature\n","\n","    def get_llm(self):\n","        return OpenAI(\n","            temperature=self.temperature,\n","            model_name=self.model_name\n","        )\n","\n","    def get_embeddings(self):\n","        return OpenAIEmbeddings()"],"metadata":{"id":"sAmh76q__VI2","executionInfo":{"status":"ok","timestamp":1736745086432,"user_tz":-330,"elapsed":508,"user":{"displayName":"M Ihshan","userId":"14788756344858455673"}}},"execution_count":37,"outputs":[]},{"cell_type":"code","source":["class HuggingFaceProvider(BaseLLMProvider):\n","    \"\"\"HuggingFace implementation with remote/local options\"\"\"\n","\n","    def __init__(self,\n","                 model_name: str = \"google/flan-t5-base\",\n","                 api_token: Optional[str] = None,\n","                 model_location: Optional[ModelLocation] = None):\n","        super().__init__(model_location or RemoteModelLocation())\n","        self.model_name = model_name\n","        self.api_token=api_token\n","\n","    def get_llm(self):\n","        model_config = self.model_location.get_model(self.model_name)\n","\n","        if model_config[\"location\"] == \"remote\":\n","            return HuggingFaceHub(\n","                repo_id=self.model_name,\n","                huggingfacehub_api_token=self.api_token,\n","                model_kwargs={\"temperature\": 0.7}\n","            )\n","        else:\n","            # For API-only usage without downloading\n","            return HuggingFaceHub(\n","                repo_id=self.model_name,\n","                huggingfacehub_api_token=self.api_token,\n","                model_kwargs={\n","                    \"temperature\": 0.7,\n","                    \"use_api\": True\n","                }\n","            )\n","\n","    def get_embeddings(self):\n","        model_config = self.model_location.get_model(self.model_name)\n","\n","        if model_config[\"location\"] == \"remote\":\n","            return HuggingFaceEmbeddings(\n","                model_name=self.model_name,\n","                model_kwargs={\"use_api\": True}\n","            )\n","        else:\n","            return HuggingFaceEmbeddings(\n","                model_name=self.model_name,\n","                cache_folder=model_config.get(\"cache_dir\")\n","            )\n"],"metadata":{"id":"A8L_ybBy_WUK","executionInfo":{"status":"ok","timestamp":1736745124582,"user_tz":-330,"elapsed":402,"user":{"displayName":"M Ihshan","userId":"14788756344858455673"}}},"execution_count":38,"outputs":[]},{"cell_type":"code","source":["API_token=\"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\"\n","model=HuggingFaceProvider(model_name=\"mistralai/Mistral-7B-v0.1\",api_token=API_token)\n","model=model.get_llm()"],"metadata":{"id":"bwe54yeS-age","executionInfo":{"status":"ok","timestamp":1736744895190,"user_tz":-330,"elapsed":371,"user":{"displayName":"M Ihshan","userId":"14788756344858455673"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["model.__call__(\"What is the strongest country?\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":139},"id":"ZPY54aVjB41g","executionInfo":{"status":"ok","timestamp":1736744926452,"user_tz":-330,"elapsed":1591,"user":{"displayName":"M Ihshan","userId":"14788756344858455673"}},"outputId":"5effe982-8a72-44b9-8108-63a8814f105f"},"execution_count":34,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'What is the strongest country? As we strive to find the answer, let us not forget that the question itself is heavily biased. It is a question that naturally assumes a country is the most powerful and can be judged by the same criteria. But the world we live in is complex and not a single country can be pigeonholed into a single answer.\\n\\nTo understand this better, let us ask a few questions. Is the United States of America the most powerful country on earth? Is China the'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":34}]},{"cell_type":"code","source":[],"metadata":{"id":"4HfEvTd---9J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class VectorStoreManager:\n","    \"\"\"Manages interactions with Pinecone vector store\"\"\"\n","\n","    def __init__(self, index_name: str, embedding_provider: BaseLLMProvider):\n","        self.index_name = index_name\n","        self.embedding_provider = embedding_provider\n","        self.vectorstore = None\n","\n","    def initialize_vectorstore(self):\n","        \"\"\"Initialize connection to Pinecone\"\"\"\n","        embeddings = self.embedding_provider.get_embeddings()\n","        self.vectorstore = Pinecone.from_existing_index(\n","            index_name=self.index_name,\n","            embedding=embeddings\n","        )\n","\n","    def similarity_search(self, query: str, k: int = 5) -> List[Dict]:\n","        \"\"\"Perform similarity search\"\"\"\n","        if not self.vectorstore:\n","            raise ValueError(\"Vector store not initialized\")\n","        return self.vectorstore.similarity_search(query, k=k)"],"metadata":{"id":"2tBk8rr1MoPD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class DocumentProcessor:\n","    \"\"\"Handles document loading and processing\"\"\"\n","\n","    def __init__(self, chunk_size: int = 1000, chunk_overlap: int = 200):\n","        self.text_splitter = RecursiveCharacterTextSplitter(\n","            chunk_size=chunk_size,\n","            chunk_overlap=chunk_overlap,\n","            length_function=len,\n","            is_separator_regex=False\n","        )\n","\n","    def load_documents(self, file_paths: List[str]) -> List[Any]:\n","        \"\"\"Load documents from various file formats\"\"\"\n","        documents = []\n","        for file_path in file_paths:\n","            file_extension = Path(file_path).suffix.lower()\n","\n","            try:\n","                if file_extension == '.pdf':\n","                    loader = PyPDFLoader(file_path)\n","                elif file_extension == '.txt':\n","                    loader = TextLoader(file_path)\n","                elif file_extension == '.csv':\n","                    loader = CSVLoader(file_path)\n","                elif file_extension in ['.doc', '.docx']:\n","                    loader = UnstructuredWordDocumentLoader(file_path)\n","                else:\n","                    raise ValueError(f\"Unsupported file format: {file_extension}\")\n","\n","                documents.extend(loader.load())\n","\n","            except Exception as e:\n","                logging.error(f\"Error loading document {file_path}: {str(e)}\")\n","                continue\n","\n","        return documents\n","\n","    def process_documents(self, documents: List[Any]) -> List[Any]:\n","        \"\"\"Split documents into chunks\"\"\"\n","        return self.text_splitter.split_documents(documents)\n"],"metadata":{"id":"t3oh2hs16ijl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["os.getcwd()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"HAgGXLsUWwV9","executionInfo":{"status":"ok","timestamp":1736749825486,"user_tz":-330,"elapsed":359,"user":{"displayName":"M Ihshan","userId":"14788756344858455673"}},"outputId":"1e165d64-a324-4996-f19f-9beb3acd6bed"},"execution_count":46,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":46}]},{"cell_type":"code","source":["\n","class RAGVectorStore:\n","    \"\"\"Manages vector store operations for RAG\"\"\"\n","\n","    def __init__(self,\n","                 index_name: str,\n","                 embedding_provider: Any,\n","                 pinecone_api_key: str,\n","                 pinecone_env: str):\n","        self.index_name = index_name\n","        self.embedding_provider = embedding_provider\n","        self.pinecone_api_key = pinecone_api_key\n","        self.pinecone_env = pinecone_env\n","        self.vectorstore = None\n","\n","    def initialize_vectorstore(self):\n","        \"\"\"Initialize Pinecone vector store\"\"\"\n","        pinecone.init(\n","            api_key=self.pinecone_api_key,\n","            environment=self.pinecone_env\n","        )\n","\n","        self.vectorstore = Pinecone.from_existing_index(\n","            index_name=self.index_name,\n","            embedding=self.embedding_provider.get_embeddings()\n","        )\n","\n","    def add_documents(self, documents: List[Any]):\n","        \"\"\"Add documents to vector store\"\"\"\n","        if not self.vectorstore:\n","            raise ValueError(\"Vector store not initialized\")\n","\n","        self.vectorstore.add_documents(documents)\n","\n","    def similarity_search(self, query: str, k: int = 4) -> List[Any]:\n","        \"\"\"Perform similarity search\"\"\"\n","        if not self.vectorstore:\n","            raise ValueError(\"Vector store not initialized\")\n","\n","        return self.vectorstore.similarity_search(query, k=k)"],"metadata":{"id":"KU4yor5-6dn1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","class RAGLLMManager:\n","    \"\"\"Manager for RAG operations with LLM integration\"\"\"\n","\n","    def __init__(\n","        self,\n","        llm_provider: Any,\n","        vector_store: Any,\n","        memory_size: int = 5\n","    ):\n","        self.llm_provider = llm_provider\n","        self.vector_store = vector_store\n","        self.memory = ConversationBufferMemory(\n","            memory_key=\"chat_history\",\n","            return_messages=True,\n","            k=memory_size\n","        )\n","        self.qa_chain = None\n","\n","        # Default RAG prompt template\n","        self.default_template = \"\"\"\n","        You are a helpful AI assistant. Use the following pieces of context to answer the question at the end.\n","        If you don't know the answer, just say that you don't know, don't try to make up an answer.\n","\n","        Context: {context}\n","\n","        Chat History: {chat_history}\n","\n","        Question: {question}\n","\n","        Helpful Answer:\"\"\"\n","\n","    def initialize(self):\n","        \"\"\"Initialize the RAG system\"\"\"\n","        self.vector_store.initialize_vectorstore()\n","\n","        prompt = PromptTemplate(\n","            input_variables=[\"context\", \"chat_history\", \"question\"],\n","            template=self.default_template\n","        )\n","\n","        self.qa_chain = RetrievalQA.from_chain_type(\n","            llm=self.llm_provider.get_llm(),\n","            chain_type=\"stuff\",\n","            retriever=self.vector_store.vectorstore.as_retriever(),\n","            memory=None,\n","            chain_type_kwargs={\n","                \"prompt\": None\n","            }\n","        )\n","\n","    # def process_and_add_documents(self,\n","    #                             file_paths: List[str],\n","    #                             processor: DocumentProcessor):\n","    #     \"\"\"Process and add documents to the vector store\"\"\"\n","    #     documents = processor.load_documents(file_paths)\n","    #     chunks = processor.process_documents(documents)\n","    #     self.vector_store.add_documents(chunks)\n","\n","    async def get_response(self,\n","                          question: str,\n","                          return_source_documents: bool = False) -> Union[str, Dict]:\n","        \"\"\"Get response using RAG\"\"\"\n","        if not self.qa_chain:\n","            raise ValueError(\"RAG system not initialized\")\n","\n","        result = await self.qa_chain.arun(\n","            query=question,\n","            return_source_documents=return_source_documents\n","        )\n","\n","        if return_source_documents:\n","            return {\n","                \"answer\": result[\"result\"],\n","                \"sources\": [doc.metadata for doc in result[\"source_documents\"]]\n","            }\n","\n","        return result\n"],"metadata":{"id":"GV8tO8n66X9L","executionInfo":{"status":"ok","timestamp":1736749545745,"user_tz":-330,"elapsed":454,"user":{"displayName":"M Ihshan","userId":"14788756344858455673"}}},"execution_count":41,"outputs":[]},{"cell_type":"code","source":["class LLMManager:\n","    \"\"\"Main class for managing LLM operations\"\"\"\n","\n","    def __init__(\n","        self,\n","        llm_provider: BaseLLMProvider,\n","        vector_store_manager: VectorStoreManager,\n","        default_prompt_template: Optional[str] = None\n","    ):\n","        self.llm_provider = llm_provider\n","        self.vector_store_manager = vector_store_manager\n","        self.llm_chain = None\n","        self.default_prompt_template = default_prompt_template or \"\"\"\n","        Context: {context}\n","        Question: {question}\n","        Please provide a detailed answer:\n","        \"\"\"\n","\n","    def initialize(self):\n","        \"\"\"Initialize LLM chain and vector store\"\"\"\n","        llm = self.llm_provider.get_llm()\n","        prompt = PromptTemplate(\n","            input_variables=[\"context\", \"question\"],\n","            template=self.default_prompt_template\n","        )\n","        self.llm_chain = LLMChain(llm=llm, prompt=prompt)\n","        self.vector_store_manager.initialize_vectorstore()\n","\n","    def switch_llm_provider(self, new_provider: BaseLLMProvider):\n","        \"\"\"Switch to a different LLM provider\"\"\"\n","        self.llm_provider = new_provider\n","        self.initialize()\n","\n","    async def get_response(self, question: str, use_context: bool = True) -> str:\n","        \"\"\"Get response from LLM with optional context from vector store\"\"\"\n","        if not self.llm_chain:\n","            raise ValueError(\"LLM chain not initialized\")\n","\n","        context = \"\"\n","        if use_context:\n","            relevant_docs = self.vector_store_manager.similarity_search(question)\n","            context = \"\\n\".join([doc.page_content for doc in relevant_docs])\n","\n","        response = await self.llm_chain.arun(\n","            context=context,\n","            question=question\n","        )\n","        return response\n"],"metadata":{"id":"CuPv-JnMMhnp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["async def main():\n","    # Initialize providers\n","    embedding_provider = OpenAIProvider()  # Or HuggingFaceProvider\n","    llm_provider = HuggingFaceProvider(model_name=\"mistralai/Mistral-7B-v0.1\",api_token=API_token)\n","\n","    # Initialize vector store\n","    vector_store = RAGVectorStore(\n","        index_name=\"my-rag-index\",\n","        embedding_provider=embedding_provider,\n","        pinecone_api_key=os.getenv(\"PINECONE_API_KEY\"),\n","        pinecone_env=os.getenv(\"PINECONE_ENV\")\n","    )\n","\n","    # Initialize RAG manager\n","    rag_manager = RAGLLMManager(\n","        llm_provider=llm_provider,\n","        vector_store=vector_store\n","    )\n","    rag_manager.initialize()\n","\n","    # # Initialize document processor\n","    # processor = DocumentProcessor(\n","    #     chunk_size=1000,\n","    #     chunk_overlap=200\n","    # )\n","\n","    # # Add documents\n","    # file_paths = [\"document1.pdf\", \"document2.txt\"]\n","    # rag_manager.process_and_add_documents(file_paths, processor)\n","\n","    # Get response\n","    response = await rag_manager.get_response(\n","        \"What is the main topic discussed in the documents?\",\n","        return_source_documents=True\n","    )\n","\n","    print(\"Answer:\", response[\"answer\"])\n","    print(\"Sources:\", response[\"sources\"])\n"],"metadata":{"id":"reJ7VBHg77VL","executionInfo":{"status":"ok","timestamp":1736749966506,"user_tz":-330,"elapsed":461,"user":{"displayName":"M Ihshan","userId":"14788756344858455673"}}},"execution_count":47,"outputs":[]}]}